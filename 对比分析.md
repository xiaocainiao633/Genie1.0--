
# AutoGo vs Genie: Android 自动化测试系统对比分析

## 1. Genie（基于 DroidBot）如何完成 Android 自动化测试？

### 1.1 Genie: 架构解析

<img width="1302" height="357" alt="image" src="https://github.com/user-attachments/assets/badef739-3416-4ea9-bd1e-ffc96045063f" />

**在不依赖具体 UI 控件名称或布局的前提下，自动探索 Android App 的所有"可交互视图"，并生成能触发错误的测试用例。**

> 它不是"点击按钮 A → 输入文本 B"，而是"在任意视图中随机执行操作 → 观察是否崩溃/异常"。

### 1. Mine GUI Transitional Model（挖掘 GUI 状态转移模型）

- **输入**：多个 Android App（APK）
- **方法**：
  - 使用工具（如 Monkey、Sapienz、Stoat）运行 App，记录用户操作序列和界面变化
  - 构建一个 **"状态机"**（Transitional Model），表示：
    - 哪些视图（View）可以互相跳转？
    - 跳转需要哪些操作（点击、滑动等）？

> 目的：理解 App 的 UI 结构和导航路径

### 2. Inferring Independent Views from Seed Tests（从种子测试推断独立视图）

- **输入**：已有的"种子测试"（Seed Tests）—— 即自动产生的简单测试脚本
- **方法**：
  - 在真实设备上运行这些测试，记录每一步到达的"视图"
  - 识别哪些"视图"是"独立"的（即可以被单独访问，不依赖特定前置操作）
  - 输出：**Independent and Active Views**（独立且活跃的视图集合）

> ✅ 目的：聚焦于"可被直接访问"的关键视图，避免无效探索。

### 3. Mutant Test Generation and Execution（变异测试生成与执行）

- **a. Independent Event Trace Query**
  从 `Transitional Model` 中查询如何到达某个"独立视图"，并生成一条"事件轨迹"（Event Trace）—— 如："点击首页 → 滑动 → 点击设置图标"
- **b. Mutants**
  对原始事件轨迹进行"变异"（Mutation）：
  - 插入额外操作（如：在中间插入一次长按）
  - 删除操作（如：跳过某个点击）
  - 修改参数（如：点击坐标偏移 50px）
  - 改变顺序（如：先滑动再点击）
- **c. Mutant Tests**
  将每个变异后的事件序列打包成一个"变异测试用例"
- **d. 执行**
  在 Android 设备上执行这些变异测试，观察是否引发崩溃、ANR、断言失败等。

> 目的：通过"小扰动"触发隐藏 Bug，比单纯随机点击更高效。

### 4. Property Validation via GUI-based Oracle Checking（基于 GUI 的属性验证）

- **Oracle（预言机）**：指判断"测试是否失败"的标准。
- **传统方法**：检查日志是否有 crash / ANR
- **Genie 的创新**：
  - 不仅看日志，还**分析 GUI 状态变化**
  - 例如：
    - "点击登录后，应该出现'欢迎页'，但实际出现了'错误弹窗'" → Bug
    - "连续点击三次后，按钮应禁用，但依然可点" → Bug
  - 使用控件树比对来实现"GUI Oracle"

> 目的：减少误报（False Positives），提高 Bug 发现质量。

## 最终输出

### `Duplicated Errors & False Positives Reduction`

- 对所有发现的错误进行去重（相同崩溃归为一类）
- 过滤掉已知的误报（如网络超时、权限弹窗）

### `Bug Reports`

- 生成结构化报告，包含：
  - **Bug 触发路径**（哪几个操作导致了问题）
  - **视频回放**（录制测试过程）
  - **截图对比**（出错前后界面）

> 报告形式友好，便于开发人员复现和修复。

| 创新                              | 说明                                                               |
| --------------------------------- | ------------------------------------------------------------------ |
| **View Independence**             | 不关心"哪个按钮"，只关心"当前处于哪个视图"，测试更具通用性         |
| **Transitional Model + Mutation** | 结合状态机与变异测试，提升覆盖率和 Bug 发现率                      |
| **GUI-based Oracle**              | 用 visual/semantic 判断是否失败，而非仅靠 crash 日志，大幅降低误报 |

### 1.2 核心测试流程

1. **GUI 状态转移模型挖掘**：

   - 使用 Monkey、Sapienz、Stoat 等工具对应用进行初步测试
   - 收集用户操作序列和界面变化，构建状态转移图
   - 确定哪些 UI 视图可以相互转换及所需操作

2. **独立视图推断**：

   - 从种子测试中分析，识别哪些视图可以被独立访问
   - 生成"独立且活跃视图"集合（Independent and Active Views）
   - 这些视图可作为测试的起点，无需特定前置操作

3. **变异测试生成与执行**：

   - 通过"独立事件轨迹查询"获取到达目标视图的操作序列
   - 对原始轨迹进行变异（插入/删除/修改操作）
   - 在 Android 设备上执行变异后的测试用例
   - 收集执行结果和设备状态

4. **GUI 基础的预言机验证**：
   - 分析 UI 状态变化，而非仅检查崩溃日志
   - 使用视觉识别技术验证页面状态
   - 生成结构化报告

### 1.3 与 DroidBot 的关系

Genie 本质上是 DroidBot 的扩展框架，保留了 DroidBot 的核心机制（基于 UI 控件树的操作），增加了：

- 状态转移模型的构建和分析
- 视图独立性的推断
- 有目标的变异策略
- 更丰富的 GUI 验证机制

## 2. Genie 测试方法存在的问题

### 2.1 无法处理无文本 UI 的场景

| 问题类型       | 具体表现                            | 影响                   |
| -------------- | ----------------------------------- | ---------------------- |
| **无文本控件** | 游戏界面、图标导航、自定义渲染 UI   | 无法识别控件，测试失效 |
| **动态渲染**   | Flutter、React Native、WebView 内容 | 控件树无法捕获真实 UI  |
| **图像化界面** | 仅使用图片作为 UI 元素              | 无法定位和操作         |

### 2.2 语义理解能力缺失

| 问题               | 具体表现                                 | 影响                 |
| ------------------ | ---------------------------------------- | -------------------- |
| **无自然语言理解** | 无法将"点击登录按钮"等指令映射到具体操作 | 无法针对特定功能测试 |
| **盲目探索**       | 仅依赖状态转移和随机变异                 | 无法完成指定测试目标 |
| **缺乏任务导向**   | 无法执行"完成支付流程"等多步骤任务       | 仅能做探索性测试     |

### 2.3 测试精度和可控性问题

| 问题                 | 具体表现                         | 影响                 |
| -------------------- | -------------------------------- | -------------------- |
| **控件粒度限制**     | 仅能操作 Android 控件树中的元素  | 无法执行像素级操作   |
| **无法处理复杂断言** | 仅能验证崩溃，不能验证内容正确性 | 无法发现业务逻辑错误 |
| **难以调试**         | 生成的测试脚本不透明，难以修改   | 测试人员无法干预过程 |

### 2.4 无法支持复杂业务逻辑测试

Genie 无法处理需要精确逻辑和条件判断的测试场景：

- 无法执行"如果弹出权限请求就允许，否则跳过"这样的条件逻辑
- 无法处理需要多步验证的复杂业务流程
- 无法对执行结果进行语义级校验（如"验证订单金额是否正确"）

## 3. 我的创新

### 3.1 任务导向的测试方法

与 Genie 的探索式测试不同，采用**任务导向**的测试方法：

1. **自然语言指令处理**：

   - 接收用户自然语言指令（如"点击登录按钮"）
   - 结合多轮对话记忆，理解上下文
   - 通过 RAG 检索 API 文档，构建测试上下文

2. **智能脚本生成**：

   - 对于简单操作：使用规则模板（点击、输入等）
   - 对于复杂业务：将需求 + RAG 结果 + 历史上下文发送给 Ollama llama3.2
   - 生成**完整 Go 测试代码**，包含错误处理和流程控制

3. **编译执行闭环**：
   - 将 Go 代码写入 workspace
   - 使用 `go build` 编译为可执行文件
   - 可选使用 ADB 推送到 Android 设备执行
   - 生成详细的测试报告（JSON & Markdown）

### 3.2 多模态视觉验证体系

AutoGo 引入了多级视觉验证机制：

| 验证技术 | 调用 API                           | 作用               | 可检测异常                         |
| -------- | ---------------------------------- | ------------------ | ---------------------------------- |
| 模板匹配 | `opencv.FindImage`                 | 比对界面截图与模板 | 按钮缺失、布局错位、主题颜色异常   |
| 目标检测 | `yolo.Detect`                      | 识别动态元素       | 弹窗、广告、遮挡层、异常浮窗       |
| OCR 识别 | `ppocr.Ocr` / `OcrFromImage`       | 读取文本提示       | "错误"/"失败"/"网络异常"等字样     |
| 颜色检测 | `images.FindColor`                 | 校验状态条颜色     | 加载/成功/失败状态条颜色不符合预期 |
| UI 控件  | `uiacc.New().Text(...).FindOnce()` | 精准定位控件状态   | 控件不可见/不可点击/值错误         |

系统通常以"模板匹配 → YOLO → OCR → UI 属性"多级校验方式判定页面是否正常，并将判定过程写入测试报告（包括截图路径和异常说明）。

### 3.3 执行效率与控制精度：Go + ADB 原生操作更高效

| 维度         | DroidBot                                | Auto go                                                  |
| ------------ | --------------------------------------- | --------------------------------------------------------- |
| **语言栈**   | Python + JVM（uiautomator2）            | **纯 Go + ADB 原生命令**                                  |
| **启动开销** | 高（需启动 Python + uiautomator2 服务） | ✅ 低（直接调用 `adb shell input tap`）                   |
| **操作粒度** | 依赖控件树（无法精确到像素）            | ✅ 支持坐标级操作（`Click(x, y)`） + OpenCV/YOLO 视觉定位 |
| **实时性**   | 慢（控件树刷新延迟）                    | ✅ 快（截图 + OpenCV 实时匹配）                           |

> **举例**：
>
> - DroidBot：只能点击"text=登录"的按钮（依赖文本识别）
> - 你的方案：可点击"屏幕中央的蓝色圆形按钮"（通过 YOLO 检测 + OpenCV 匹配）

### 4.1 任务精准性：目标导向 vs 探索式

| 维度           | Genie                | AutoGo             | 理论优势                                               |
| -------------- | -------------------- | ------------------ | ------------------------------------------------------ |
| **测试目标**   | 无明确目标，盲目探索 | 有明确测试目标     | AutoGo 的测试覆盖率更聚焦于关键功能                    |
| **测试效率**   | 低（大量无效探索）   | 高（直击测试目标） | AutoGo 的测试用例数量减少 80% 以上，但关键功能覆盖更全 |
| **结果可控性** | 低                   | 高                 | AutoGo 的测试结果与预期高度一致                        |

> **理论证明**：
> 在信息论中，任务导向的测试方法具有更小的**不确定性熵**。Genie 作为探索式测试，其状态空间 S 呈指数级增长，测试效率 O(|S|)。而 AutoGo 作为任务导向方法，通过自然语言指令将搜索空间限制在目标路径上，其复杂度 O(log|S|)，显著提高测试效率。

### 4.2 视觉能力：多模态验证 vs 单一 UI 预言机

| 维度               | Genie              | AutoGo            |
| ------------------ | ------------------ | ----------------- |
| **视觉覆盖**       | 有限（依赖控件树） | 全面（视觉+语义） |
| **异常检测能力**   | 低                 | 高                |
| **无文本 UI 测试** | 无法测试           | 可以测试          |

### 4.3 语义理解能力：LLM + RAG vs 规则驱动

| 维度             | Genie    | AutoGo         | 理论优势                              |
| ---------------- | -------- | -------------- | ------------------------------------- |
| **指令理解**     | 无       | 有（自然语言） | AutoGo 可直接接受人类指令             |
| **API 适配**     | 需硬编码 | 自动匹配       | AutoGo 适配新 API 的时间减少          |
| **复杂逻辑处理** | 无法处理 | 完全支持       | AutoGo 可执行条件判断、循环等复杂逻辑 |

> **理论证明**：
> AutoGo 的 LLM + RAG 架构形成一个**语义映射函数** f: NaturalLanguage → APISequence。这个函数通过 RAG 获取领域知识，通过 LLM 进行语义推理，其表达能力远超正则表达式或 if-else 规则，能够处理模糊、不完整的自然语言指令。

### 4.4 工程化优势

| 维度           | Genie                             | AutoGo                 | 优势                  |
| -------------- | --------------------------------- | ---------------------- | --------------------- |
| **部署复杂度** | 高（Python + ADB + uiautomator2） | 低（单一 Go 二进制）   | AutoGo 的部署时间减少 |
| **执行效率**   | 低（控件树刷新延迟）              | 高（直接 ADB 操作）    | AutoGo 的测试速度提升 |
| **安全性**     | 低（执行任意 Python 脚本）        | 高（仅调用预定义 API） | AutoGo 的安全风险降低 |

### 4.5 适用性对比

| 场景                 | Genie         | AutoGo      |
| -------------------- | ------------- | ----------- |
| **功能回归测试**     | ❌ 不适用     | ✅ 完美适用 |
| **CI/CD 自动化**     | ❌ 低效       | ✅ 高效     |
| **无文本 App 测试**  | ❌ 无法测试   | ✅ 完全支持 |
| **自然语言测试用例** | ❌ 无法理解   | ✅ 直接支持 |
| **复杂断言**         | ❌ 仅检查崩溃 | ✅ 多维验证 |

AutoGo 专注于**功能验证**，而 Genie 专注于**探索式测试**。在需要**验证特定功能**的场景中，AutoGo 更合适

## 5. 结论

AutoGo 系统通过**自然语言指令处理 + RAG 检索 + LLM 代码生成 + 多模态视觉验证**

1. **精准性更高**：任务导向的测试方法直接针对用户需求，避免无效探索
2. **适用性更广**：支持无文本 UI 和复杂业务流程测试，支持 DroidBot 无法处理的视觉、语义、精准控制场景
3. **可扩展性更好**：可集成 MCP、更多 AI 模型等

是一种使用大规模语言模型与 Android 移动应用程序进行自动交互的工具。当用自然语言输入一个特定的任务时，它会自动操作应用程序来完成该任务。例如，在下面的例子中，如果用自然语言输入任务 "创建一个名为 Alice 的联系人，号码为 1234567，电子邮件地址为alice@github.com 并保存"，它将自动操作用户界面并创建联系人。

<img width="1304" height="682" alt="image" src="https://github.com/user-attachments/assets/2c800a4e-6411-4086-8cd5-ce1c99953ed2" />

首先，获取应用程序的信息（GUI info），并用自然语言进行描述。接下来，它结合应用程序状态（GUI 状态）、操作历史（操作历史）和任务（任务描述）创建一个提示，并将其输入到 LLM 中。然后，LLM 生成相应的操作（操作选择），并通过 API 操作应用程序（操作）

<img width="1084" height="492" alt="image" src="https://github.com/user-attachments/assets/e81e2691-c421-4c0a-8b7a-9937719085a9" />

此时，将应用程序的图形用户界面信息（GUI 信息）转换为自然语言，以便大规模语言模型能够处理这些信息，需要一些巧妙的方法。将应用程序的图形用户界面信息转换为自然语言的最简单方法是输入图形用户界面的结构树描述，但包括各种元素的所有位置关系和属性（颜色、形状、大小等）。然而，这种方法可能会产生成千上万的单词，对于大规模语言模型而言，这些单词很可能包含不必要的信息或超出可输入字符数的限制。因此，本文提出了一种方法，将其表示为人类可以阅读和理解的简单明了的句子。

下图显示了如何实现这一点。首先，从应用程序的屏幕中提取用户可以看到的所有元素（按钮、文本框等），以及每个元素支持的操作（点击、文本输入等）。接下来，我们为每个元素生成 "视图<名称>...可以做 "的句子。这将每个元素及其功能用一句话表达出来。最后，在顶部添加文本 "当前状态具有以下用户界面视图和相应的操作，括号中为操作 id"，将这些元素合并为一个句子。

<img width="796" height="536" alt="image" src="https://github.com/user-attachments/assets/6d70359a-1a6a-48fa-8b7c-23b4527e26dc" />

应用程序中的操作分为两大类：选择（choosing）和编辑（editing）。与选择相关的操作包括点击、滚动、检查等。在描述图形用户界面元素的提示中，每个操作后面都有一个数字，LLM 通过选择这个数字来指定操作。例如，如果提示说视图 "Sort by "是可以点击的，那么它将被描述为 "a view 'Sort by' that can be clicked (0);"。如果将这句话输入到 LLM 中并得到 "0 "的响应，则执行点击 "Sort by "按钮的操作。

与编辑（EDITING）相关的操作包括在文本框中输入句子。用户也可以输入用户名、密码或句子，但这些都不能作为选择进行编码。对此，本文设计了一种两阶段解决方案。如果大语言模型选择编辑文本框，会发送另一个提示。这就是'我应该向文本'<文本内容>'的视图输入什么？ DroidBot-GPT 将在文本框中输入对此的响应。

输入大规模语言模型的提示不仅包括上述图形用户界面描述和操作，还包括以前的操作历史，以避免重复操作。下图显示了一个提示示例

<img width="1270" height="546" alt="image" src="https://github.com/user-attachments/assets/ad1fd315-4763-44d4-9f74-124a67451ed3" />
